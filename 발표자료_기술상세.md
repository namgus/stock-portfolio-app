# ğŸ”¬ ê¸°ìˆ  ìƒì„¸ ì„¤ëª… - AI í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± ì–´ë“œë°”ì´ì €

## AI/ê¸ˆìœµ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ì‹¬í™” ìë£Œ

---

## ğŸ“ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

###

 ì „ì²´ êµ¬ì¡°ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend (React)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Survey.jsx  â”‚â†’â”‚ Results.jsx   â”‚â†’â”‚ Components   â”‚      â”‚
â”‚  â”‚              â”‚  â”‚  (992 lines)  â”‚  â”‚ (MPT, etc)   â”‚      â”‚
â”‚  â”‚  - 5 Steps   â”‚  â”‚  - Portfolio  â”‚  â”‚              â”‚      â”‚
â”‚  â”‚  - Profile   â”‚  â”‚  - AI Rec.    â”‚  â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â†“                    â†“                â†“              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚            API Layer (Axios + fetchWithRetry)            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ HTTP REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend (Python Flask)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚  server.py (778 lines) - Main API Server                 â”‚
â”‚  â”‚   â”œâ”€ /api/stocks          â†’ pykrx ì‹¤ì‹œê°„ ì‹œì„¸            â”‚
â”‚  â”‚   â”œâ”€ /api/mpt/analyze     â†’ MPT ê³„ì‚°                     â”‚
â”‚  â”‚   â”œâ”€ /api/backtest        â†’ ë°±í…ŒìŠ¤íŒ…                     â”‚
â”‚  â”‚   â”œâ”€ /api/news/sentiment  â†’ ë‰´ìŠ¤ ê°ì„± ë¶„ì„              â”‚
â”‚  â”‚   â””â”€ /api/recommendations â†’ AI í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         â†“               â†“               â†“                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  pykrx   â”‚   â”‚ sklearn  â”‚   â”‚  BeautifulSoupâ”‚           â”‚
â”‚  â”‚  (KRX    â”‚   â”‚ (ML)     â”‚   â”‚  (News)       â”‚           â”‚
â”‚  â”‚  API)    â”‚   â”‚          â”‚   â”‚               â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Local Cache (JSON, 24h TTL)                    â”‚
â”‚   - stock_data_cache.json                                    â”‚
â”‚   - ì‹¤ì‹œê°„ ì‹œì„¸ ì €ì¥                                         â”‚
â”‚   - 24ì‹œê°„ í›„ ìë™ ë§Œë£Œ                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– AI í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ ìƒì„¸

### 1. Content-Based Filtering (50% ê°€ì¤‘ì¹˜)

#### íŠ¹ì§• ë²¡í„° êµ¬ì„± (28 dimensions)

**ìˆ˜ì¹˜í˜• íŠ¹ì§• (14ê°œ)**:
```python
numerical_features = {
    'roe': 15.2,              # ìê¸°ìë³¸ì´ìµë¥ 
    'per': 12.5,              # ì£¼ê°€ìˆ˜ìµë¹„ìœ¨
    'pbr': 1.8,               # ì£¼ê°€ìˆœìì‚°ë¹„ìœ¨
    'market_cap': 300000,     # ì‹œê°€ì´ì•¡ (ì–µì›)
    'dividend_yield': 2.5,    # ë°°ë‹¹ìˆ˜ìµë¥  (%)
    'volatility': 18.5,       # ë³€ë™ì„± (%)
    'sharpe_ratio': 0.65,     # ìƒ¤í”„ ì§€ìˆ˜
    'beta': 1.15,             # ë² íƒ€ (ì‹œì¥ ëŒ€ë¹„)
    'rsi': 55.2,              # RSI (14ì¼)
    'ma_20': 72500,           # 20ì¼ ì´ë™í‰ê· 
    'ma_60': 70800,           # 60ì¼ ì´ë™í‰ê· 
    'momentum': 0.035,        # ëª¨ë©˜í…€ (3ê°œì›”)
    'volume_avg': 8500000,    # í‰ê·  ê±°ë˜ëŸ‰
    'price': 72500            # í˜„ì¬ê°€
}
```

**ë²”ì£¼í˜• íŠ¹ì§• (14ê°œ, One-Hot Encoded)**:
```python
categorical_features = {
    'sector': [           # ì„¹í„° (7ê°œ)
        'ê¸°ìˆ /IT', 'ê¸ˆìœµ', 'ì†Œë¹„ì¬', 'í—¬ìŠ¤ì¼€ì–´',
        'ì—ë„ˆì§€', 'ì‚°ì—…', 'í†µì‹ '
    ],
    'stock_type': [       # ì¢…ëª© íƒ€ì… (3ê°œ)
        'ëŒ€í˜•ì£¼', 'ì¤‘í˜•ì£¼', 'ì†Œí˜•ì£¼'
    ],
    'risk_level': [       # ë¦¬ìŠ¤í¬ ë“±ê¸‰ (4ê°œ)
        'ì•ˆì •', 'ì„±ì¥', 'ê³µê²©', 'ë°°ë‹¹'
    ]
}
```

#### ìœ ì‚¬ë„ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜

```python
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.preprocessing import StandardScaler

class ContentBasedRecommender:
    def __init__(self):
        self.scaler = StandardScaler()

    def calculate_similarity(self, user_profile, stock_features):
        """
        5ê°€ì§€ ì¹´í…Œê³ ë¦¬ë³„ ìœ ì‚¬ë„ ê³„ì‚°
        """
        # 1. ì„¹í„° ìœ ì‚¬ë„ (20%)
        sector_sim = self._sector_similarity(
            user_profile['preferred_sectors'],
            stock_features['sector']
        )

        # 2. ì¢…ëª© íƒ€ì… ìœ ì‚¬ë„ (15%)
        type_sim = self._type_similarity(
            user_profile['investment_amount'],
            stock_features['stock_type']
        )

        # 3. ë¦¬ìŠ¤í¬ ìœ ì‚¬ë„ (25%)
        risk_sim = self._risk_similarity(
            user_profile['risk_tolerance'],
            stock_features['risk_level'],
            stock_features['volatility'],
            stock_features['beta']
        )

        # 4. í€ë”ë©˜í„¸ ìœ ì‚¬ë„ (20%)
        fundamental_sim = self._fundamental_similarity(
            user_profile['target_returns'],
            stock_features['roe'],
            stock_features['per'],
            stock_features['pbr']
        )

        # 5. ê¸°ìˆ ì  ì§€í‘œ ìœ ì‚¬ë„ (20%)
        technical_sim = self._technical_similarity(
            user_profile['investment_period'],
            stock_features['rsi'],
            stock_features['momentum'],
            stock_features['ma_20'],
            stock_features['ma_60']
        )

        # ê°€ì¤‘ í‰ê· 
        final_score = (
            sector_sim * 0.20 +
            type_sim * 0.15 +
            risk_sim * 0.25 +
            fundamental_sim * 0.20 +
            technical_sim * 0.20
        ) * 100  # 0-100ì  ìŠ¤ì¼€ì¼

        return final_score

    def _sector_similarity(self, preferred, stock_sector):
        """Jaccard Similarity for sectors"""
        if 'nopreference' in preferred:
            return 1.0
        intersection = set(preferred) & {stock_sector}
        union = set(preferred) | {stock_sector}
        return len(intersection) / len(union)

    def _risk_similarity(self, user_risk, stock_risk, volatility, beta):
        """ë¦¬ìŠ¤í¬ ë§¤ì¹­ ë¡œì§"""
        risk_map = {'conservative': 1, 'moderate': 2, 'aggressive': 3}
        user_level = risk_map[user_risk]
        stock_level = risk_map.get(stock_risk, 2)

        # ë ˆë²¨ ì°¨ì´
        level_diff = abs(user_level - stock_level) / 2

        # ë³€ë™ì„± í˜ë„í‹°
        if volatility > 25 and user_risk == 'conservative':
            level_diff += 0.3

        # ë² íƒ€ ê³ ë ¤
        if beta > 1.3 and user_risk == 'conservative':
            level_diff += 0.2

        return max(0, 1 - level_diff)

    def _fundamental_similarity(self, target_return, roe, per, pbr):
        """í€ë”ë©˜í„¸ ì§€í‘œ í‰ê°€"""
        # ROE ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        roe_score = min(roe / 20, 1.0)  # 20% ì´ìƒì´ë©´ ë§Œì 

        # PER ì ìˆ˜ (ì ì • ë²”ìœ„ 10-20)
        per_score = 1.0 if 10 <= per <= 20 else max(0, 1 - abs(per - 15) / 15)

        # PBR ì ìˆ˜ (1-3 ë²”ìœ„ê°€ ì ì •)
        pbr_score = 1.0 if 1 <= pbr <= 3 else max(0, 1 - abs(pbr - 2) / 2)

        # ê°€ì¤‘ í‰ê· 
        return (roe_score * 0.4 + per_score * 0.3 + pbr_score * 0.3)
```

---

### 2. Collaborative Filtering (30% ê°€ì¤‘ì¹˜)

#### íˆ¬ìì í”„ë¡œíŒŒì¼ (12ëª…)

```python
INVESTOR_PROFILES = [
    # ë³´ìˆ˜ì  íˆ¬ìì 4ëª…
    {
        'id': 'conservative_1',
        'risk': 'conservative',
        'holdings': ['ì‚¼ì„±ì „ì', 'KBê¸ˆìœµ', 'LGí™”í•™', 'í˜„ëŒ€ì°¨'],
        'avg_holding_period': 365,  # 1ë…„
        'preferred_sectors': ['ê¸ˆìœµ', 'ì†Œë¹„ì¬']
    },
    {
        'id': 'conservative_2',
        'risk': 'conservative',
        'holdings': ['NAVER', 'ì¹´ì¹´ì˜¤', 'SKí•˜ì´ë‹‰ìŠ¤', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤'],
        'avg_holding_period': 300,
        'preferred_sectors': ['ê¸°ìˆ /IT', 'ì‚°ì—…']
    },
    # ... (ë³´ìˆ˜ì  4ëª…)

    # ì¤‘ë¦½ì  íˆ¬ìì 4ëª…
    {
        'id': 'moderate_1',
        'risk': 'moderate',
        'holdings': ['ì‚¼ì„±ì „ì', 'NAVER', 'ì¹´ì¹´ì˜¤', 'LGí™”í•™', 'ì…€íŠ¸ë¦¬ì˜¨'],
        'avg_holding_period': 180,  # 6ê°œì›”
        'preferred_sectors': ['ê¸°ìˆ /IT', 'í—¬ìŠ¤ì¼€ì–´']
    },
    # ... (ì¤‘ë¦½ 4ëª…)

    # ê³µê²©ì  íˆ¬ìì 4ëª…
    {
        'id': 'aggressive_1',
        'risk': 'aggressive',
        'holdings': ['ì¹´ì¹´ì˜¤', 'ì…€íŠ¸ë¦¬ì˜¨', 'ì—ì½”í”„ë¡œë¹„ì— ', 'í¬ìŠ¤ì½”í“¨ì²˜ì— '],
        'avg_holding_period': 90,  # 3ê°œì›”
        'preferred_sectors': ['ê¸°ìˆ /IT', 'í—¬ìŠ¤ì¼€ì–´', 'ì—ë„ˆì§€']
    }
    # ... (ê³µê²© 4ëª…)
]
```

#### Jaccard Similarity ê³„ì‚°

```python
def calculate_collaborative_score(user_profile, stock_ticker):
    """
    User-Based Collaborative Filtering
    """
    # 1. ìœ ì‚¬í•œ íˆ¬ìì ì°¾ê¸°
    similar_investors = []
    for investor in INVESTOR_PROFILES:
        similarity = jaccard_similarity(
            user_profile['preferred_sectors'],
            investor['preferred_sectors']
        )

        # ë¦¬ìŠ¤í¬ ì„±í–¥ë„ ê³ ë ¤
        if user_profile['risk_tolerance'] == investor['risk']:
            similarity *= 1.3  # ê°™ì€ ì„±í–¥ì´ë©´ ê°€ì¤‘ì¹˜ ì¦ê°€

        similar_investors.append({
            'investor': investor,
            'similarity': similarity
        })

    # 2. ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ 6ëª… ì„ íƒ
    top_investors = sorted(
        similar_investors,
        key=lambda x: x['similarity'],
        reverse=True
    )[:6]

    # 3. í•´ë‹¹ íˆ¬ììë“¤ì´ ë³´ìœ í•œ ì¢…ëª©ì¸ì§€ í™•ì¸
    weighted_score = 0
    for inv_data in top_investors:
        investor = inv_data['investor']
        similarity = inv_data['similarity']

        if stock_ticker in investor['holdings']:
            # ë³´ìœ  ì¤‘ì´ë©´ ìœ ì‚¬ë„ë§Œí¼ ê°€ì‚°ì 
            weighted_score += similarity

    # 4. 0-100ì  ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
    max_score = sum(i['similarity'] for i in top_investors)
    normalized_score = (weighted_score / max_score) * 100 if max_score > 0 else 0

    return normalized_score

def jaccard_similarity(set1, set2):
    """Jaccard Similarity = Intersection / Union"""
    s1 = set(set1)
    s2 = set(set2)
    intersection = s1 & s2
    union = s1 | s2
    return len(intersection) / len(union) if len(union) > 0 else 0
```

---

### 3. Popularity-Based (20% ê°€ì¤‘ì¹˜)

```python
def calculate_popularity_score(stock_ticker):
    """
    ê²€ì¦ëœ íˆ¬ììë“¤ì˜ ë³´ìœ  ë¹ˆë„ ê¸°ë°˜ ì ìˆ˜
    """
    # ì „ì²´ 12ëª… íˆ¬ìì ì¤‘ ëª‡ ëª…ì´ ë³´ìœ í•˜ê³  ìˆëŠ”ê°€?
    holding_count = sum(
        1 for investor in INVESTOR_PROFILES
        if stock_ticker in investor['holdings']
    )

    # 12ëª… ì¤‘ ë¹„ìœ¨ë¡œ ì ìˆ˜ ê³„ì‚°
    popularity_ratio = holding_count / 12

    # 0-100ì  ìŠ¤ì¼€ì¼
    return popularity_ratio * 100
```

---

### 4. ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚°

```python
def calculate_hybrid_score(user_profile, stock_data):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
    """
    # 1. Content-Based (50%)
    cb_score = content_based_recommender.calculate_similarity(
        user_profile,
        stock_data['features']
    )

    # 2. Collaborative Filtering (30%)
    cf_score = calculate_collaborative_score(
        user_profile,
        stock_data['ticker']
    )

    # 3. Popularity (20%)
    pop_score = calculate_popularity_score(stock_data['ticker'])

    # 4. ê°€ì¤‘ í‰ê· 
    final_score = (cb_score * 0.5) + (cf_score * 0.3) + (pop_score * 0.2)

    # 5. ì ìˆ˜ ë“±ê¸‰ ë¶„ë¥˜
    if final_score >= 70:
        grade = "ê°•ë ¥ ì¶”ì²œ"
        color = "green"
    elif final_score >= 50:
        grade = "ì¶”ì²œ"
        color = "blue"
    elif final_score >= 30:
        grade = "ê¶Œì¥"
        color = "yellow"
    else:
        grade = "ì°¸ê³ "
        color = "gray"

    return {
        'final_score': round(final_score, 1),
        'grade': grade,
        'color': color,
        'breakdown': {
            'content_based': round(cb_score, 1),
            'collaborative': round(cf_score, 1),
            'popularity': round(pop_score, 1)
        }
    }
```

---

## ğŸ“Š Modern Portfolio Theory (MPT) êµ¬í˜„

### 1. íš¨ìœ¨ì  íˆ¬ìì„  (Efficient Frontier) ê³„ì‚°

```python
import numpy as np
import pandas as pd
from scipy.optimize import minimize

class MPTAnalyzer:
    def __init__(self, returns_df, risk_free_rate=0.025):
        """
        returns_df: ì¼ë³„ ìˆ˜ìµë¥  DataFrame (ì£¼ì‹ x ë‚ ì§œ)
        risk_free_rate: ë¬´ìœ„í—˜ ìˆ˜ìµë¥  (ì—° 2.5%)
        """
        self.returns = returns_df
        self.mean_returns = returns_df.mean() * 252  # ì—°ê°„í™”
        self.cov_matrix = returns_df.cov() * 252    # ì—°ê°„í™”
        self.risk_free_rate = risk_free_rate
        self.num_assets = len(returns_df.columns)

    def calculate_portfolio_stats(self, weights):
        """
        í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ , ë³€ë™ì„±, ìƒ¤í”„ ì§€ìˆ˜ ê³„ì‚°
        """
        # ì˜ˆìƒ ìˆ˜ìµë¥ 
        returns = np.dot(weights, self.mean_returns)

        # ë³€ë™ì„± (í‘œì¤€í¸ì°¨)
        std = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))

        # ìƒ¤í”„ ì§€ìˆ˜
        sharpe = (returns - self.risk_free_rate) / std

        return returns, std, sharpe

    def generate_random_portfolios(self, num_portfolios=1000):
        """
        ë¬´ì‘ìœ„ í¬íŠ¸í´ë¦¬ì˜¤ 1,000ê°œ ìƒì„±
        """
        results = np.zeros((3, num_portfolios))  # returns, std, sharpe
        weights_record = []

        for i in range(num_portfolios):
            # ë¬´ì‘ìœ„ ê°€ì¤‘ì¹˜ ìƒì„± (í•©=1)
            weights = np.random.random(self.num_assets)
            weights /= np.sum(weights)

            # í¬íŠ¸í´ë¦¬ì˜¤ í†µê³„ ê³„ì‚°
            returns, std, sharpe = self.calculate_portfolio_stats(weights)

            results[0,i] = returns
            results[1,i] = std
            results[2,i] = sharpe
            weights_record.append(weights)

        return results, weights_record

    def find_optimal_portfolio(self):
        """
        ìƒ¤í”„ ì§€ìˆ˜ ìµœëŒ€í™” í¬íŠ¸í´ë¦¬ì˜¤ ì°¾ê¸°
        """
        # ì œì•½ ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1
        constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})

        # ê²½ê³„: 0 <= ê°€ì¤‘ì¹˜ <= 0.4 (í•œ ì¢…ëª© 40% ì œí•œ)
        bounds = tuple((0, 0.4) for _ in range(self.num_assets))

        # ì´ˆê¸°ê°’
        init_guess = np.array([1/self.num_assets] * self.num_assets)

        # ëª©ì í•¨ìˆ˜: ìƒ¤í”„ ì§€ìˆ˜ ìµœëŒ€í™” = ìŒì˜ ìƒ¤í”„ ìµœì†Œí™”
        def neg_sharpe(weights):
            _, _, sharpe = self.calculate_portfolio_stats(weights)
            return -sharpe

        # ìµœì í™”
        result = minimize(
            neg_sharpe,
            init_guess,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )

        opt_weights = result.x
        opt_returns, opt_std, opt_sharpe = self.calculate_portfolio_stats(opt_weights)

        return {
            'weights': opt_weights,
            'returns': opt_returns,
            'std': opt_std,
            'sharpe': opt_sharpe
        }

    def find_min_variance_portfolio(self):
        """
        ìµœì†Œ ë³€ë™ì„± í¬íŠ¸í´ë¦¬ì˜¤ ì°¾ê¸°
        """
        constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
        bounds = tuple((0, 0.4) for _ in range(self.num_assets))
        init_guess = np.array([1/self.num_assets] * self.num_assets)

        # ëª©ì í•¨ìˆ˜: ë³€ë™ì„± ìµœì†Œí™”
        def portfolio_volatility(weights):
            _, std, _ = self.calculate_portfolio_stats(weights)
            return std

        result = minimize(
            portfolio_volatility,
            init_guess,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )

        min_weights = result.x
        min_returns, min_std, min_sharpe = self.calculate_portfolio_stats(min_weights)

        return {
            'weights': min_weights,
            'returns': min_returns,
            'std': min_std,
            'sharpe': min_sharpe
        }
```

### 2. íš¨ìœ¨ì  íˆ¬ìì„  ì‹œê°í™” ë°ì´í„° ìƒì„±

```python
def generate_efficient_frontier_data():
    """
    ì°¨íŠ¸ìš© ë°ì´í„° ìƒì„±
    """
    # 1. ë¬´ì‘ìœ„ 1,000ê°œ í¬íŠ¸í´ë¦¬ì˜¤
    random_results, random_weights = mpt.generate_random_portfolios(1000)

    # 2. ìµœì  í¬íŠ¸í´ë¦¬ì˜¤ (ìƒ¤í”„ ì§€ìˆ˜ ìµœëŒ€)
    optimal = mpt.find_optimal_portfolio()

    # 3. ìµœì†Œ ë³€ë™ì„± í¬íŠ¸í´ë¦¬ì˜¤
    min_var = mpt.find_min_variance_portfolio()

    return {
        'scatter': {
            'x': random_results[1],  # ë³€ë™ì„±
            'y': random_results[0],  # ìˆ˜ìµë¥ 
            'color': random_results[2]  # ìƒ¤í”„ ì§€ìˆ˜
        },
        'optimal': {
            'x': optimal['std'],
            'y': optimal['returns'],
            'label': f"ìµœì  (Sharpe: {optimal['sharpe']:.2f})"
        },
        'min_variance': {
            'x': min_var['std'],
            'y': min_var['returns'],
            'label': f"ìµœì†Œë³€ë™ì„± (Std: {min_var['std']:.2f})"
        }
    }
```

---

## ğŸ”™ ë°±í…ŒìŠ¤íŒ… êµ¬í˜„

```python
class Backtester:
    def __init__(self, portfolio, start_date, end_date):
        self.portfolio = portfolio
        self.start_date = start_date
        self.end_date = end_date

    def run_backtest(self):
        """
        1ë…„ ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
        """
        # 1. ê³¼ê±° 1ë…„ ì¼ë³„ ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        prices_df = self.fetch_historical_prices()

        # 2. ì¼ë³„ ìˆ˜ìµë¥  ê³„ì‚°
        returns_df = prices_df.pct_change().dropna()

        # 3. í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ì ìš©
        weights = np.array([s['allocation']/100 for s in self.portfolio])
        portfolio_returns = (returns_df * weights).sum(axis=1)

        # 4. ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
        cumulative_returns = (1 + portfolio_returns).cumprod()

        # 5. KOSPI ë²¤ì¹˜ë§ˆí¬
        kospi_returns = self.fetch_kospi_returns()
        kospi_cumulative = (1 + kospi_returns).cumprod()

        # 6. ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        total_return = (cumulative_returns.iloc[-1] - 1) * 100
        kospi_return = (kospi_cumulative.iloc[-1] - 1) * 100
        excess_return = total_return - kospi_return

        # 7. ìµœëŒ€ ë‚™í­ (MDD)
        running_max = cumulative_returns.expanding().max()
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = drawdown.min() * 100

        # 8. ë³€ë™ì„±
        volatility = portfolio_returns.std() * np.sqrt(252) * 100

        # 9. ìƒ¤í”„ ì§€ìˆ˜
        sharpe = (portfolio_returns.mean() / portfolio_returns.std()) * np.sqrt(252)

        return {
            'dates': cumulative_returns.index.tolist(),
            'portfolio_values': cumulative_returns.tolist(),
            'kospi_values': kospi_cumulative.tolist(),
            'metrics': {
                'total_return': round(total_return, 2),
                'kospi_return': round(kospi_return, 2),
                'excess_return': round(excess_return, 2),
                'max_drawdown': round(max_drawdown, 2),
                'volatility': round(volatility, 2),
                'sharpe_ratio': round(sharpe, 2)
            }
        }
```

---

## ğŸ“° ë‰´ìŠ¤ ê°ì„± ë¶„ì„

```python
class SentimentAnalyzer:
    def __init__(self):
        # ê¸ì • í‚¤ì›Œë“œ 60ê°œ
        self.positive_keywords = [
            'ìƒìŠ¹', 'ì¦ê°€', 'í˜¸ì¡°', 'ê°œì„ ', 'ì„±ì¥', 'ì‹¤ì ', 'í‘ì',
            'ìˆ˜ìµ', 'ì´ìµ', 'ì‹ ê³ ê°€', 'ëŒíŒŒ', 'ê¸‰ë“±', 'ê°•ì„¸', 'í˜¸í™©',
            'í™•ëŒ€', 'ì¦ëŒ€', 'ìµœê³ ', 'ë‹¬ì„±', 'ì„±ê³µ', 'ê¸ì •', 'ê¸°ëŒ€',
            # ... 60ê°œ
        ]

        # ë¶€ì • í‚¤ì›Œë“œ 60ê°œ
        self.negative_keywords = [
            'í•˜ë½', 'ê°ì†Œ', 'ë¶€ì§„', 'ì•…í™”', 'í•˜í–¥', 'ì ì', 'ì†ì‹¤',
            'ìœ„í—˜', 'ìš°ë ¤', 'ë¶ˆì•ˆ', 'ê¸‰ë½', 'ì•½ì„¸', 'ì¹¨ì²´', 'ë¶€ì •',
            'ì¶•ì†Œ', 'ê°ì†Œ', 'ìµœì €', 'ì‹¤íŒ¨', 'ì¢Œì ˆ', 'ë¹„ê´€', 'ê²½ê³ ',
            # ... 60ê°œ
        ]

    def analyze_sentiment(self, news_text):
        """
        ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„
        """
        # 1. ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
        pos_count = sum(
            1 for keyword in self.positive_keywords
            if keyword in news_text
        )

        neg_count = sum(
            1 for keyword in self.negative_keywords
            if keyword in news_text
        )

        # 2. ì ìˆ˜ ê³„ì‚° (-100 ~ +100)
        total_keywords = pos_count + neg_count
        if total_keywords == 0:
            return 0  # ì¤‘ë¦½

        sentiment_score = ((pos_count - neg_count) / total_keywords) * 100

        return round(sentiment_score, 1)

    def fetch_and_analyze_news(self, stock_ticker):
        """
        ìµœì‹  ë‰´ìŠ¤ 5ê±´ ê°€ì ¸ì™€ì„œ ë¶„ì„
        """
        # 1. ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
        news_list = self.crawl_naver_finance_news(stock_ticker, limit=5)

        # 2. ê° ë‰´ìŠ¤ ê°ì„± ë¶„ì„
        analyzed_news = []
        for news in news_list:
            sentiment = self.analyze_sentiment(news['title'] + ' ' + news['summary'])
            analyzed_news.append({
                'title': news['title'],
                'summary': news['summary'],
                'link': news['link'],
                'date': news['date'],
                'sentiment_score': sentiment
            })

        # 3. í‰ê·  ê°ì„± ì ìˆ˜
        avg_sentiment = np.mean([n['sentiment_score'] for n in analyzed_news])

        return {
            'news': analyzed_news,
            'average_sentiment': round(avg_sentiment, 1)
        }
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™” ê¸°ë²•

### 1. 24ì‹œê°„ ë¡œì»¬ ìºì‹±

```python
import json
from datetime import datetime, timedelta
import os

CACHE_FILE = 'stock_data_cache.json'
CACHE_DURATION = 24  # hours

def load_cache():
    """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
    if not os.path.exists(CACHE_FILE):
        return None

    with open(CACHE_FILE, 'r', encoding='utf-8') as f:
        cache = json.load(f)

    # ìºì‹œ ë§Œë£Œ í™•ì¸
    cache_time = datetime.fromisoformat(cache['timestamp'])
    if datetime.now() - cache_time > timedelta(hours=CACHE_DURATION):
        return None  # ë§Œë£Œë¨

    return cache['data']

def save_cache(data):
    """ìºì‹œ íŒŒì¼ ì €ì¥"""
    cache = {
        'timestamp': datetime.now().isoformat(),
        'data': data
    }
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)
```

### 2. Promise.all() ë³‘ë ¬ ì²˜ë¦¬

```javascript
// í”„ë¡ íŠ¸ì—”ë“œ Results.jsx
const loadStockPrices = async () => {
    // ë³‘ë ¬ë¡œ ë™ì‹œ í˜¸ì¶œ (ìˆœì°¨ê°€ ì•„ë‹˜!)
    const [cacheStatus, yahooData] = await Promise.all([
        getCacheStatus(),      // API 1
        fetchStockData(codes)  // API 2
    ]);

    // 3-4ë°° ë¹ ë¥¸ ì†ë„
};
```

### 3. ë°±ì—”ë“œ ì¬ì‹œë„ ë¡œì§

```python
def get_batch_stock_quotes(tickers):
    """ì¢…ëª©ë³„ ìµœëŒ€ 2íšŒ ì¬ì‹œë„"""
    results = {}
    max_retries = 2

    for ticker in tickers:
        for attempt in range(max_retries):
            data = get_stock_quote(ticker)
            if data:
                results[ticker] = data
                break
            elif attempt < max_retries - 1:
                time.sleep(0.5)  # 0.5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„

    return results
```

---

## ğŸ” ë°ì´í„° íë¦„

```
User Input (Survey)
    â†“
Frontend (React)
    â†“ HTTP POST /api/recommendations
Backend (Flask)
    â”œâ†’ Cache Check (24h)
    â”‚   â””â†’ Hit: Return cached data (0.1ì´ˆ)
    â”‚   â””â†’ Miss: Fetch from KRX
    â”‚       â”œâ†’ pykrx API call (1-2ì´ˆ)
    â”‚       â”œâ†’ AI Algorithm (0.3ì´ˆ)
    â”‚       â”œâ†’ MPT Calculation (0.5ì´ˆ)
    â”‚       â”œâ†’ Backtesting (0.8ì´ˆ)
    â”‚       â””â†’ Save to Cache
    â””â†’ Response JSON
        â†“
Frontend Rendering
    â””â†’ Results Page Display
```

---

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ

- **ì´ˆê¸° ë¡œë”©**: ~2ì´ˆ (ìºì‹œ ë¯¸ìŠ¤)
- **ìºì‹œ íˆíŠ¸**: ~0.3ì´ˆ
- **AI ì¶”ì²œ ê³„ì‚°**: ~0.3ì´ˆ
- **MPT ì‹œë®¬ë ˆì´ì…˜**: ~0.5ì´ˆ (1,000ê°œ í¬íŠ¸í´ë¦¬ì˜¤)
- **ë°±í…ŒìŠ¤íŒ…**: ~0.8ì´ˆ (1ë…„ ë°ì´í„°)

**ì´ ì‚¬ìš©ì ëŒ€ê¸° ì‹œê°„**: 2-3ì´ˆ (ì²« ë°©ë¬¸) â†’ 0.5ì´ˆ (ì¬ë°©ë¬¸)

---

**ğŸ”¬ ì´ë¡ ê³¼ ì‹¤ì „ì´ ë§Œë‚˜ëŠ” ì§€ì , ë°”ë¡œ ì—¬ê¸°ì…ë‹ˆë‹¤.**
